{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import spacy\n",
    "import string\n",
    "import pickle as pkl\n",
    "from nltk import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "    data = []\n",
    "    for f in listdir(path):\n",
    "        if f.endswith('.txt'):\n",
    "            file = open(path + '/' + f, 'r')\n",
    "            text = file.read()\n",
    "            data.append(text)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_neg_path = '/Users/Near_Mac/Desktop/NLP/HW1/aclImdb/train/neg'\n",
    "train_neg = read_files(train_neg_path);\n",
    "\n",
    "train_pos_path = '/Users/Near_Mac/Desktop/NLP/HW1/aclImdb/train/pos'\n",
    "train_pos = read_files(train_pos_path);\n",
    "\n",
    "test_neg_path = '/Users/Near_Mac/Desktop/NLP/HW1/aclImdb/test/neg'\n",
    "test_neg = read_files(test_neg_path);\n",
    "\n",
    "test_pos_path = '/Users/Near_Mac/Desktop/NLP/HW1/aclImdb/test/pos'\n",
    "test_pos = read_files(test_pos_path);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_split = 10000\n",
    "\n",
    "train_data = train_pos[:train_split] + train_neg[:train_split]\n",
    "train_targets = [1] * train_split + [0] * train_split\n",
    "\n",
    "val_data = train_pos[train_split:] + train_neg[train_split:]\n",
    "val_targets = [1] * len(train_pos[train_split:]) + [0] * len(train_neg[train_split:])\n",
    "\n",
    "test_data = test_pos + test_neg\n",
    "test_targets = [1] * len(test_pos) + [0] * len(test_neg)\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def tokenize(sent, punc=False):\n",
    "    if(punc == False):\n",
    "        tokens = tokenizer(sent)\n",
    "        return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    else:\n",
    "        tokens = tokenizer(sent)\n",
    "        return [token.text.lower() for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'is', 'looking', 'at', 'buying', 'u.k.', 'startup', 'for', '1', 'billion']\n",
      "['apple', 'is', 'looking', 'at', 'buying', 'u.k.', 'startup', 'for', '$', '1', 'billion', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_np = tokenize('Apple is looking at buying U.K. startup for $1 billion.',False)\n",
    "print (tokens_np)\n",
    "\n",
    "tokens_wp = tokenize('Apple is looking at buying U.K. startup for $1 billion.',True)\n",
    "print (tokens_wp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_ngrams(tokens, n):\n",
    "    tokens_ngrams = ngrams(tokens, n)\n",
    "    return [tokens for tokens in tokens_ngrams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_wp_1g = tokenize_ngrams(tokens_wp, 1)\n",
    "tokens_wp_2g = tokenize_ngrams(tokens_wp, 2)\n",
    "tokens_wp_3g = tokenize_ngrams(tokens_wp, 3)\n",
    "tokens_wp_4g = tokenize_ngrams(tokens_wp, 4)\n",
    "\n",
    "tokens_np_1g = tokenize_ngrams(tokens_np, 1)\n",
    "tokens_np_2g = tokenize_ngrams(tokens_np, 2)\n",
    "tokens_np_3g = tokenize_ngrams(tokens_np, 3)\n",
    "tokens_np_4g = tokenize_ngrams(tokens_np, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apple',), ('is',), ('looking',), ('at',), ('buying',), ('u.k.',), ('startup',), ('for',), ('$',), ('1',), ('billion',), ('.',)]\n",
      "\n",
      "[('apple', 'is'), ('is', 'looking'), ('looking', 'at'), ('at', 'buying'), ('buying', 'u.k.'), ('u.k.', 'startup'), ('startup', 'for'), ('for', '$'), ('$', '1'), ('1', 'billion'), ('billion', '.')]\n",
      "\n",
      "[('apple', 'is', 'looking'), ('is', 'looking', 'at'), ('looking', 'at', 'buying'), ('at', 'buying', 'u.k.'), ('buying', 'u.k.', 'startup'), ('u.k.', 'startup', 'for'), ('startup', 'for', '$'), ('for', '$', '1'), ('$', '1', 'billion'), ('1', 'billion', '.')]\n",
      "\n",
      "[('apple', 'is', 'looking', 'at'), ('is', 'looking', 'at', 'buying'), ('looking', 'at', 'buying', 'u.k.'), ('at', 'buying', 'u.k.', 'startup'), ('buying', 'u.k.', 'startup', 'for'), ('u.k.', 'startup', 'for', '$'), ('startup', 'for', '$', '1'), ('for', '$', '1', 'billion'), ('$', '1', 'billion', '.')]\n",
      "\n",
      "[('apple',), ('is',), ('looking',), ('at',), ('buying',), ('u.k.',), ('startup',), ('for',), ('1',), ('billion',)]\n",
      "\n",
      "[('apple', 'is'), ('is', 'looking'), ('looking', 'at'), ('at', 'buying'), ('buying', 'u.k.'), ('u.k.', 'startup'), ('startup', 'for'), ('for', '1'), ('1', 'billion')]\n",
      "\n",
      "[('apple', 'is', 'looking'), ('is', 'looking', 'at'), ('looking', 'at', 'buying'), ('at', 'buying', 'u.k.'), ('buying', 'u.k.', 'startup'), ('u.k.', 'startup', 'for'), ('startup', 'for', '1'), ('for', '1', 'billion')]\n",
      "\n",
      "[('apple', 'is', 'looking', 'at'), ('is', 'looking', 'at', 'buying'), ('looking', 'at', 'buying', 'u.k.'), ('at', 'buying', 'u.k.', 'startup'), ('buying', 'u.k.', 'startup', 'for'), ('u.k.', 'startup', 'for', '1'), ('startup', 'for', '1', 'billion')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokens_wp_1g)\n",
    "print()\n",
    "print(tokens_wp_2g)\n",
    "print()\n",
    "print(tokens_wp_3g)\n",
    "print()\n",
    "print(tokens_wp_4g)\n",
    "print()\n",
    "\n",
    "print(tokens_np_1g)\n",
    "print()\n",
    "print(tokens_np_2g)\n",
    "print()\n",
    "print(tokens_np_3g)\n",
    "print()\n",
    "print(tokens_np_4g)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_ngrams_dataset(dataset, n, punc):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize_ngrams(tokenize(sample, punc), n)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "#print (\"Tokenizing val data\")\n",
    "#val_data_tokens, _ = tokenize_ngrams_dataset(val_data,2,True)\n",
    "\n",
    "#print (\"Tokenizing test data\")\n",
    "#test_data_tokens, _ = tokenize_ngrams_dataset(test_data,2,True)\n",
    "\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_ngrams_dataset(train_data,2,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('for', 'a'), ('a', 'movie'), ('movie', 'that'), ('that', 'gets'), ('gets', 'no'), ('no', 'respect'), ('respect', 'there'), ('there', 'sure'), ('sure', 'are'), ('are', 'a')]\n"
     ]
    }
   ],
   "source": [
    "print(all_train_tokens[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('for', 'a'), ('a', 'movie'), ('movie', 'that'), ('that', 'gets'), ('gets', 'no'), ('no', 'respect'), ('respect', 'there'), ('there', 'sure'), ('sure', 'are'), ('are', 'a'), ('a', 'lot'), ('lot', 'of'), ('of', 'memorable'), ('memorable', 'quotes'), ('quotes', 'listed'), ('listed', 'for'), ('for', 'this'), ('this', 'gem'), ('gem', '.'), ('.', 'imagine'), ('imagine', 'a'), ('a', 'movie'), ('movie', 'where'), ('where', 'joe'), ('joe', 'piscopo'), ('piscopo', 'is'), ('is', 'actually'), ('actually', 'funny'), ('funny', '!'), ('!', 'maureen'), ('maureen', 'stapleton'), ('stapleton', 'is'), ('is', 'a'), ('a', 'scene'), ('scene', 'stealer'), ('stealer', '.'), ('.', 'the'), ('the', 'moroni'), ('moroni', 'character'), ('character', 'is'), ('is', 'an'), ('an', 'absolute'), ('absolute', 'scream'), ('scream', '.'), ('.', 'watch'), ('watch', 'for'), ('for', 'alan'), ('alan', '\"'), ('\"', 'the'), ('the', 'skipper'), ('skipper', '\"'), ('\"', 'hale'), ('hale', 'jr'), ('jr', '.'), ('.', 'as'), ('as', 'a'), ('a', 'police'), ('police', 'sgt'), ('sgt', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(train_data_tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    for p in [True, False]:\n",
    "        val_data_tokens, _ = tokenize_ngrams_dataset(val_data,i,p)\n",
    "        test_data_tokens, _ = tokenize_ngrams_dataset(test_data,i,p)\n",
    "        train_data_tokens, all_train_tokens = tokenize_ngrams_dataset(train_data,i,p)\n",
    "        if(p == True):\n",
    "            pkl.dump(val_data_tokens, open(\"./pkl/val_tokens_wp_\"+str(i)+\".p\", \"wb\"))\n",
    "            pkl.dump(test_data_tokens, open(\"./pkl/test_tokens_wp_\"+str(i)+\".p\", \"wb\"))\n",
    "            pkl.dump(train_data_tokens, open(\"./pkl/train_tokens_wp_\"+str(i)+\".p\", \"wb\"))\n",
    "            pkl.dump(all_train_tokens, open(\"./pkl/all_train_tokens_wp_\"+str(i)+\".p\", \"wb\"))\n",
    "        else:\n",
    "            pkl.dump(val_data_tokens, open(\"./pkl/val_tokens_np_\"+str(i)+\".p\", \"wb\"))\n",
    "            pkl.dump(test_data_tokens, open(\"./pkl/test_tokens_np_\"+str(i)+\".p\", \"wb\"))\n",
    "            pkl.dump(train_data_tokens, open(\"./pkl/train_tokens_np_\"+str(i)+\".p\", \"wb\"))\n",
    "            pkl.dump(all_train_tokens, open(\"./pkl/all_train_tokens_np_\"+str(i)+\".p\", \"wb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(train_targets, open(\"./pkl/train_targets.p\", \"wb\"))\n",
    "pkl.dump(val_targets, open(\"./pkl/val_targets.p\", \"wb\"))\n",
    "pkl.dump(test_targets, open(\"./pkl/test_targets.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
