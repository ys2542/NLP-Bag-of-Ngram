{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MovieReviewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moviereview_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_targets = pkl.load(open(\"./pkl/train_targets.p\",\"rb\"))\n",
    "val_targets = pkl.load(open(\"./pkl/val_targets.p\", \"rb\"))\n",
    "test_targets = pkl.load(open(\"./pkl/test_targets.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_tokens_wp_1 = pkl.load(open(\"./pkl/train_tokens_wp_1.p\", \"rb\"))\n",
    "all_train_tokens_wp_1 = pkl.load(open(\"./pkl/all_train_tokens_wp_1.p\", \"rb\"))\n",
    "val_data_tokens_wp_1 = pkl.load(open(\"./pkl/val_tokens_wp_1.p\", \"rb\"))\n",
    "test_data_tokens_wp_1 = pkl.load(open(\"./pkl/test_tokens_wp_1.p\", \"rb\"))\n",
    "\n",
    "train_data_tokens_wp_2 = pkl.load(open(\"./pkl/train_tokens_wp_2.p\", \"rb\"))\n",
    "all_train_tokens_wp_2 = pkl.load(open(\"./pkl/all_train_tokens_wp_2.p\", \"rb\"))\n",
    "val_data_tokens_wp_2 = pkl.load(open(\"./pkl/val_tokens_wp_2.p\", \"rb\"))\n",
    "test_data_tokens_wp_2 = pkl.load(open(\"./pkl/test_tokens_wp_2.p\", \"rb\"))\n",
    "\n",
    "train_data_tokens_wp_3 = pkl.load(open(\"./pkl/train_tokens_wp_3.p\", \"rb\"))\n",
    "all_train_tokens_wp_3 = pkl.load(open(\"./pkl/all_train_tokens_wp_3.p\", \"rb\"))\n",
    "val_data_tokens_wp_3 = pkl.load(open(\"./pkl/val_tokens_wp_3.p\", \"rb\"))\n",
    "test_data_tokens_wp_3 = pkl.load(open(\"./pkl/test_tokens_wp_3.p\", \"rb\"))\n",
    "\n",
    "train_data_tokens_wp_4 = pkl.load(open(\"./pkl/train_tokens_wp_4.p\", \"rb\"))\n",
    "all_train_tokens_wp_4 = pkl.load(open(\"./pkl/all_train_tokens_wp_4.p\", \"rb\"))\n",
    "val_data_tokens_wp_4 = pkl.load(open(\"./pkl/val_tokens_wp_4.p\", \"rb\"))\n",
    "test_data_tokens_wp_4 = pkl.load(open(\"./pkl/test_tokens_wp_4.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_tokens_np_1 = pkl.load(open(\"./pkl/train_tokens_np_1.p\", \"rb\"))\n",
    "all_train_tokens_np_1 = pkl.load(open(\"./pkl/all_train_tokens_np_1.p\", \"rb\"))\n",
    "val_data_tokens_np_1 = pkl.load(open(\"./pkl/val_tokens_np_1.p\", \"rb\"))\n",
    "test_data_tokens_np_1 = pkl.load(open(\"./pkl/test_tokens_np_1.p\", \"rb\"))\n",
    "\n",
    "train_data_tokens_np_2 = pkl.load(open(\"./pkl/train_tokens_np_2.p\", \"rb\"))\n",
    "all_train_tokens_np_2 = pkl.load(open(\"./pkl/all_train_tokens_np_2.p\", \"rb\"))\n",
    "val_data_tokens_np_2 = pkl.load(open(\"./pkl/val_tokens_np_2.p\", \"rb\"))\n",
    "test_data_tokens_np_2 = pkl.load(open(\"./pkl/test_tokens_np_2.p\", \"rb\"))\n",
    "\n",
    "train_data_tokens_np_3 = pkl.load(open(\"./pkl/train_tokens_np_3.p\", \"rb\"))\n",
    "all_train_tokens_np_3 = pkl.load(open(\"./pkl/all_train_tokens_np_3.p\", \"rb\"))\n",
    "val_data_tokens_np_3 = pkl.load(open(\"./pkl/val_tokens_np_3.p\", \"rb\"))\n",
    "test_data_tokens_np_3 = pkl.load(open(\"./pkl/test_tokens_np_3.p\", \"rb\"))\n",
    "\n",
    "train_data_tokens_np_4 = pkl.load(open(\"./pkl/train_tokens_np_4.p\", \"rb\"))\n",
    "all_train_tokens_np_4 = pkl.load(open(\"./pkl/all_train_tokens_np_4.p\", \"rb\"))\n",
    "val_data_tokens_np_4 = pkl.load(open(\"./pkl/val_tokens_np_4.p\", \"rb\"))\n",
    "test_data_tokens_np_4 = pkl.load(open(\"./pkl/test_tokens_np_4.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_tokens_wp = []\n",
    "train_data_tokens_wp.append(train_data_tokens_wp_1)\n",
    "train_data_tokens_wp.append(train_data_tokens_wp_2)\n",
    "train_data_tokens_wp.append(train_data_tokens_wp_3)\n",
    "train_data_tokens_wp.append(train_data_tokens_wp_4)\n",
    "\n",
    "val_data_tokens_wp = []\n",
    "val_data_tokens_wp.append(val_data_tokens_wp_1)\n",
    "val_data_tokens_wp.append(val_data_tokens_wp_2)\n",
    "val_data_tokens_wp.append(val_data_tokens_wp_3)\n",
    "val_data_tokens_wp.append(val_data_tokens_wp_4)\n",
    "\n",
    "test_data_tokens_wp = []\n",
    "test_data_tokens_wp.append(test_data_tokens_wp_1)\n",
    "test_data_tokens_wp.append(test_data_tokens_wp_2)\n",
    "test_data_tokens_wp.append(test_data_tokens_wp_3)\n",
    "test_data_tokens_wp.append(test_data_tokens_wp_4)\n",
    "\n",
    "all_train_tokens_wp = []\n",
    "all_train_tokens_wp.append(all_train_tokens_wp_1)\n",
    "all_train_tokens_wp.append(all_train_tokens_wp_2)\n",
    "all_train_tokens_wp.append(all_train_tokens_wp_3)\n",
    "all_train_tokens_wp.append(all_train_tokens_wp_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_tokens_np = []\n",
    "train_data_tokens_np.append(train_data_tokens_np_1)\n",
    "train_data_tokens_np.append(train_data_tokens_np_2)\n",
    "train_data_tokens_np.append(train_data_tokens_np_3)\n",
    "train_data_tokens_np.append(train_data_tokens_np_4)\n",
    "\n",
    "val_data_tokens_np = []\n",
    "val_data_tokens_np.append(val_data_tokens_np_1)\n",
    "val_data_tokens_np.append(val_data_tokens_np_2)\n",
    "val_data_tokens_np.append(val_data_tokens_np_3)\n",
    "val_data_tokens_np.append(val_data_tokens_np_4)\n",
    "\n",
    "test_data_tokens_np = []\n",
    "test_data_tokens_np.append(test_data_tokens_np_1)\n",
    "test_data_tokens_np.append(test_data_tokens_np_2)\n",
    "test_data_tokens_np.append(test_data_tokens_np_3)\n",
    "test_data_tokens_np.append(test_data_tokens_np_4)\n",
    "\n",
    "all_train_tokens_np = []\n",
    "all_train_tokens_np.append(all_train_tokens_np_1)\n",
    "all_train_tokens_np.append(all_train_tokens_np_2)\n",
    "all_train_tokens_np.append(all_train_tokens_np_3)\n",
    "all_train_tokens_np.append(all_train_tokens_np_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_select(punc, ngrams):\n",
    "    if(punc == 'w'):\n",
    "        all_train_tokens = all_train_tokens_wp[ngrams-1]\n",
    "        train_data_tokens = train_data_tokens_wp[ngrams-1]\n",
    "        val_data_tokens = val_data_tokens_wp[ngrams-1]\n",
    "        test_data_tokens = test_data_tokens_wp[ngrams-1]\n",
    "    elif(punc == 'n'):\n",
    "        all_train_tokens = all_train_tokens_np[ngrams-1]\n",
    "        train_data_tokens = train_data_tokens_np[ngrams-1]\n",
    "        val_data_tokens = val_data_tokens_np[ngrams-1]\n",
    "        test_data_tokens = test_data_tokens_np[ngrams-1]\n",
    "        \n",
    "    return all_train_tokens, train_data_tokens, val_data_tokens, test_data_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "learning_rate = 0.005\n",
    "num_epochs = 4\n",
    "\n",
    "for pc in ['w','n']:\n",
    "    for ng in [1,2,3,4]:\n",
    "        for max_vocab_size in [20000,30000]:\n",
    "            all_train_tokens, train_data_tokens, val_data_tokens, test_data_tokens = token_select(pc, ng)\n",
    "            token2id, id2token = build_vocab(all_train_tokens, max_vocab_size)\n",
    "        \n",
    "            train_data_indices = token2index_dataset(train_data_tokens, token2id)\n",
    "            val_data_indices = token2index_dataset(val_data_tokens, token2id)\n",
    "            test_data_indices = token2index_dataset(test_data_tokens, token2id)\n",
    "                \n",
    "            train_dataset = MovieReviewDataset(train_data_indices, train_targets)\n",
    "            train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       collate_fn=moviereview_collate_func,\n",
    "                                                       shuffle=True)\n",
    "\n",
    "            val_dataset = MovieReviewDataset(val_data_indices, val_targets)\n",
    "            val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        collate_fn=moviereview_collate_func,\n",
    "                                                        shuffle=True)\n",
    "\n",
    "            test_dataset = MovieReviewDataset(test_data_indices, test_targets)\n",
    "            test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        collate_fn=moviereview_collate_func,\n",
    "                                                        shuffle=False)\n",
    "            for emb_dim in [200,300]:\n",
    "                model = BagOfWords(len(id2token), emb_dim)\n",
    "                criterion = torch.nn.CrossEntropyLoss()\n",
    "                for opt in ['Adam', 'SGD']:\n",
    "                    file_path = './result/' + pc + '_' + str(ng) + '_' + str(max_vocab_size) + '_' + str(emb_dim) + '_' + opt + '.txt'\n",
    "                    f= open(file_path,\"w+\")\n",
    "                    \n",
    "                    if(opt == 'Adam'):\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                    elif(opt == 'SGD'):\n",
    "                        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "                        \n",
    "                    val_acc_epochs = []\n",
    "                    step_numbers = []\n",
    "\n",
    "                    for epoch in range(num_epochs):\n",
    "                        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "                            model.train()\n",
    "                            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = model(data_batch, length_batch)\n",
    "                            loss = criterion(outputs, label_batch)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            # validate every 100 iterations\n",
    "                            if i > 0 and i % 100 == 0:\n",
    "                                # validate\n",
    "                                val_acc = test_model(val_loader, model)\n",
    "                                val_acc_epochs.append(val_acc)\n",
    "                                step_numbers.append(i + epoch * 600)\n",
    "                                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                                           epoch+1, num_epochs, i+1, len(train_loader), val_acc), file=open(file_path, \"a\"))\n",
    "            \n",
    "                    print (\"After training for {} epochs\".format(num_epochs), file=open(file_path, \"a\"))\n",
    "                    print (\"Val Acc {}\".format(test_model(val_loader, model)), file=open(file_path, \"a\"))\n",
    "                    print (\"Test Acc {}\".format(test_model(test_loader, model)), file=open(file_path, \"a\"))\n",
    "                                \n",
    "                    #plt.plot(step_numbers, val_acc_epochs)\n",
    "                    #plt.xlabel(\"Step\")\n",
    "                    #plt.ylabel(\"Validation Accuracy\")\n",
    "                    #plt.title(\"Training curve\")\n",
    "                    #plot_path = './plot/' + pc + '_' + str(ng) + '_' + str(max_vocab_size) + '_' + str(emb_dim) + '_' + opt + '.png'\n",
    "                    #plt.savefig(plot_path)  \n",
    "                    #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "max_vocab_size = 20000\n",
    "emb_dim = 200\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "learning_rate = 0.005\n",
    "num_epochs = 5\n",
    "\n",
    "all_train_tokens, train_data_tokens, val_data_tokens, test_data_tokens = token_select('w', 1)\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size)\n",
    "train_data_indices = token2index_dataset(train_data_tokens, token2id)\n",
    "val_data_indices = token2index_dataset(val_data_tokens, token2id)\n",
    "test_data_indices = token2index_dataset(test_data_tokens, token2id)\n",
    "\n",
    "\n",
    "train_dataset = MovieReviewDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviereview_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = MovieReviewDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviereview_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieReviewDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviereview_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "val_acc_epochs = []\n",
    "step_numbers = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            val_acc_epochs.append(val_acc)\n",
    "            step_numbers.append(i + epoch * 600)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))\n",
    "\n",
    "\n",
    "plt.plot(step_numbers, val_acc_epochs)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Training curve\")\n",
    "plt.savefig(\"./plot/curve_w_1.png\")\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for pc in ['w', 'n']:\n",
    "    for ng in [1,2,3,4]:\n",
    "        for max_vocab_size in [10000, 20000, 30000]:\n",
    "            for emb_dim in [50, 100, 200]:\n",
    "                for opt in ['SGD', 'Adam']:\n",
    "                    file_path = './result/' + pc + '_' + str(ng) + '_' + str(max_vocab_size) + '_' + str(emb_dim) + '_' + opt + '.txt'\n",
    "                    f= open(file_path,\"w+\")\n",
    "                    print(\"Hello stackoverflow!\", file=open(file_path, \"a\"))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(step_numbers, val_acc_epochs)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Training curve\")\n",
    "plot_path = './plot/' + pc + '_' + str(ng) + '_' + str(max_vocab_size) + '_' + str(emb_dim) + '_' + opt + '.png'\n",
    "plt.savefig(plot_path)  \n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
